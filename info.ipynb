{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fe55e81-8c29-44e7-8a0b-d7700654f760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "# Load TorchScript model\n",
    "model = torch.jit.load('runs/train/yolo_shufflenet//weights/best.pt' , device='cuda' if torch.cuda.is_available() else 'cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c39aee4-2b6f-4c09-aae1-088b400034f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: []",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mK:\\environment\\ObjectDetection\\.venv\\Lib\\site-packages\\torchinfo\\torchinfo.py:295\u001b[0m, in \u001b[0;36mforward_pass\u001b[1;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 295\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[1;32mK:\\environment\\ObjectDetection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mK:\\environment\\ObjectDetection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__/models/yolo.py\", line 87, in forward\n    _41 = (_25).forward((_24).forward(_40, ), _32, )\n    _42 = (_27).forward((_26).forward(_41, ), )\n    return ((_28).forward(_38, _40, _42, ),)\n             ~~~~~~~~~~~~ <--- HERE\nclass Detect(Module):\n  __parameters__ = []\n  File \"code/__torch__/models/yolo.py\", line 116, in forward\n    _20 = torch.split_with_sizes(torch.sigmoid(_19), [2, 2, 3], 4)\n    xy, wh, conf, = _20\n    _21 = torch.add(torch.mul(xy, CONSTANTS.c0), CONSTANTS.c1)\n          ~~~~~~~~~ <--- HERE\n    xy0 = torch.mul(_21, torch.select(CONSTANTS.c2, 0, 0))\n    _22 = torch.pow(torch.mul(wh, CONSTANTS.c0), 2)\n\nTraceback of TorchScript, original code (most recent call last):\n/content/drive/.shortcut-targets-by-id/1hMtT1CZ8c2huhEDHcXqpEOoFN8AceyJ4/cap/yolov5/models/yolo.py(88): forward\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1726): _slow_forward\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1747): _call_impl\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1736): _wrapped_call_impl\n/content/drive/.shortcut-targets-by-id/1hMtT1CZ8c2huhEDHcXqpEOoFN8AceyJ4/cap/yolov5/models/yolo.py(146): _forward_once\n/content/drive/.shortcut-targets-by-id/1hMtT1CZ8c2huhEDHcXqpEOoFN8AceyJ4/cap/yolov5/models/yolo.py(247): forward\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1726): _slow_forward\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1747): _call_impl\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py(1736): _wrapped_call_impl\n/usr/local/lib/python3.11/dist-packages/torch/jit/_trace.py(1278): trace_module\n/usr/local/lib/python3.11/dist-packages/torch/jit/_trace.py(698): _trace_impl\n/usr/local/lib/python3.11/dist-packages/torch/jit/_trace.py(1002): trace\n/content/drive/MyDrive/cap/yolov5/export.py(274): export_torchscript\n/content/drive/MyDrive/cap/yolov5/export.py(218): outer_func\n/content/drive/MyDrive/cap/yolov5/export.py(1418): run\n/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py(116): decorate_context\n/content/drive/MyDrive/cap/yolov5/export.py(1541): main\n/content/drive/MyDrive/cap/yolov5/export.py(1546): <module>\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchinfo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[1;32m----> 3\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mK:\\environment\\ObjectDetection\\.venv\\Lib\\site-packages\\torchinfo\\torchinfo.py:223\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    216\u001b[0m validate_user_params(\n\u001b[0;32m    217\u001b[0m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[0;32m    218\u001b[0m )\n\u001b[0;32m    220\u001b[0m x, correct_input_size \u001b[38;5;241m=\u001b[39m process_input(\n\u001b[0;32m    221\u001b[0m     input_data, input_size, batch_dim, device, dtypes\n\u001b[0;32m    222\u001b[0m )\n\u001b[1;32m--> 223\u001b[0m summary_list \u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_forward_pass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m formatting \u001b[38;5;241m=\u001b[39m FormattingOptions(depth, verbose, columns, col_width, rows)\n\u001b[0;32m    227\u001b[0m results \u001b[38;5;241m=\u001b[39m ModelStatistics(\n\u001b[0;32m    228\u001b[0m     summary_list, correct_input_size, get_total_memory_used(x), formatting\n\u001b[0;32m    229\u001b[0m )\n",
      "File \u001b[1;32mK:\\environment\\ObjectDetection\\.venv\\Lib\\site-packages\\torchinfo\\torchinfo.py:304\u001b[0m, in \u001b[0;36mforward_pass\u001b[1;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    303\u001b[0m     executed_layers \u001b[38;5;241m=\u001b[39m [layer \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m summary_list \u001b[38;5;28;01mif\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mexecuted]\n\u001b[1;32m--> 304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to run torchinfo. See above stack traces for more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuted layers up to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecuted_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: []"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=(1, 3, 640, 640))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9032b7e3-c63b-483b-a8c6-f05346ae5f2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "register_forward_hook is not supported on ScriptModules",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Dummy input\u001b[39;00m\n\u001b[0;32m      2\u001b[0m input_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m640\u001b[39m, \u001b[38;5;241m640\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m flops, params \u001b[38;5;241m=\u001b[39m \u001b[43mget_model_complexity_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_strings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_per_layer_stat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mK:\\environment\\ObjectDetection\\.venv\\Lib\\site-packages\\ptflops\\flops_counter.py:94\u001b[0m, in \u001b[0;36mget_model_complexity_info\u001b[1;34m(model, input_res, print_per_layer_stat, as_strings, input_constructor, ost, verbose, ignore_modules, custom_modules_hooks, backend, flops_units, param_units, output_precision, backend_specific_config)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, nn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m FLOPS_BACKEND(backend) \u001b[38;5;241m==\u001b[39m FLOPS_BACKEND\u001b[38;5;241m.\u001b[39mPYTORCH:\n\u001b[0;32m     93\u001b[0m     flops_count, params_count \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m---> 94\u001b[0m         \u001b[43mget_flops_pytorch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_res\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mprint_per_layer_stat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m                          \u001b[49m\u001b[43minput_constructor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43most\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcustom_modules_hooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m                          \u001b[49m\u001b[43moutput_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mflops_units\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflops_units\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mparam_units\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam_units\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mextra_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend_specific_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m FLOPS_BACKEND(backend) \u001b[38;5;241m==\u001b[39m FLOPS_BACKEND\u001b[38;5;241m.\u001b[39mATEN:\n\u001b[0;32m    104\u001b[0m     flops_count, params_count \u001b[38;5;241m=\u001b[39m get_flops_aten(model, input_res,\n\u001b[0;32m    105\u001b[0m                                                print_per_layer_stat,\n\u001b[0;32m    106\u001b[0m                                                input_constructor, ost,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    111\u001b[0m                                                param_units\u001b[38;5;241m=\u001b[39mparam_units,\n\u001b[0;32m    112\u001b[0m                                                extra_config\u001b[38;5;241m=\u001b[39mbackend_specific_config)\n",
      "File \u001b[1;32mK:\\environment\\ObjectDetection\\.venv\\Lib\\site-packages\\ptflops\\pytorch_engine.py:37\u001b[0m, in \u001b[0;36mget_flops_pytorch\u001b[1;34m(model, input_res, print_per_layer_stat, input_constructor, ost, verbose, ignore_modules, custom_modules_hooks, output_precision, flops_units, param_units, extra_config)\u001b[0m\n\u001b[0;32m     35\u001b[0m flops_model \u001b[38;5;241m=\u001b[39m add_flops_counting_methods(model)\n\u001b[0;32m     36\u001b[0m flops_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m---> 37\u001b[0m \u001b[43mflops_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_flops_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43most\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43most\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mignore_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_modules\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_constructor:\n\u001b[0;32m     40\u001b[0m     batch \u001b[38;5;241m=\u001b[39m input_constructor(input_res)\n",
      "File \u001b[1;32mK:\\environment\\ObjectDetection\\.venv\\Lib\\site-packages\\ptflops\\pytorch_engine.py:211\u001b[0m, in \u001b[0;36mstart_flops_count\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstart_flops_count\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    203\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m    A method that will be available after add_flops_counting_methods() is called\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    on a desired net object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m \n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m     \u001b[43madd_batch_counter_hook_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m     seen_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21madd_flops_counter_hook_function\u001b[39m(module, ost, verbose, ignore_list):\n",
      "File \u001b[1;32mK:\\environment\\ObjectDetection\\.venv\\Lib\\site-packages\\ptflops\\pytorch_engine.py:289\u001b[0m, in \u001b[0;36madd_batch_counter_hook_function\u001b[1;34m(module)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__batch_counter_handle__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m handle \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_forward_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_counter_hook\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    290\u001b[0m module\u001b[38;5;241m.\u001b[39m__batch_counter_handle__ \u001b[38;5;241m=\u001b[39m handle\n",
      "File \u001b[1;32mK:\\environment\\ObjectDetection\\.venv\\Lib\\site-packages\\torch\\jit\\_script.py:984\u001b[0m, in \u001b[0;36m_make_fail.<locals>.fail\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfail\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not supported on ScriptModules\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: register_forward_hook is not supported on ScriptModules"
     ]
    }
   ],
   "source": [
    "# Dummy input\n",
    "input_size = (3, 640, 640)\n",
    "\n",
    "flops, params = get_model_complexity_info(model, input_size, as_strings=True, print_per_layer_stat=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eb9bd65-9734-4fd3-832e-07df87f9c353",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1       464  models.common.Conv_maxpool              [3, 16]                       \n",
      "  1                -1  1      2768  models.common.ShuffleNetV2_InvertedResidual[16, 64, 2]                   \n",
      "  2                -1  1      2528  models.common.ShuffleNetV2_InvertedResidual[64, 64, 1]                   \n",
      "  3                -1  1     14080  models.common.ShuffleNetV2_InvertedResidual[64, 128, 2]                  \n",
      "  4                -1  3     27456  models.common.ShuffleNetV2_InvertedResidual[128, 128, 1]                 \n",
      "  5                -1  1     50636  models.common.ShuffleNetV2_InvertedResidual[128, 248, 2]                 \n",
      "  6                -1  1     32612  models.common.ShuffleNetV2_InvertedResidual[248, 248, 1]                 \n",
      "  7                -1  1    136248  models.common.DWConv2D                  [248, 544, 1, 1]              \n",
      "  8                -1  1    444992  models.common.SPPF                      [544, 272, 5]                 \n",
      "  9                -1  1    856464  models.common.TransformerEncoder        [272, 272, 8, 1024, 0.1]      \n",
      " 10                -1  1     74800  models.common.DWConv2D                  [272, 272, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    129595  models.common.PEAM                      [400, 272]                    \n",
      " 14                -1  1    334560  models.common.C3                        [272, 272, 1, False]          \n",
      " 15                -1  1     37536  models.common.DWConv2D                  [272, 136, 1, 1]              \n",
      " 16                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 17           [-1, 2]  1         0  models.common.Concat                    [1]                           \n",
      " 18                -1  1     32446  models.common.PEAM                      [200, 136]                    \n",
      " 19                -1  1     84048  models.common.C3                        [136, 136, 1, False]          \n",
      " 20                -1  1     19992  models.common.DWConv2D                  [136, 136, 3, 2]              \n",
      " 21          [-1, 12]  1         0  models.common.Concat                    [1]                           \n",
      " 22                -1  1    182107  models.common.PEAM                      [536, 272]                    \n",
      " 23                -1  1    334560  models.common.C3                        [272, 272, 1, False]          \n",
      " 24                -1  1     76976  models.common.DWConv2D                  [272, 272, 3, 2]              \n",
      " 25           [-1, 9]  1         0  models.common.Concat                    [1]                           \n",
      " 26                -1  1    334148  models.common.PEAM                      [544, 544]                    \n",
      " 27                -1  1   1334976  models.common.C3                        [544, 544, 1, False]          \n",
      " 28      [19, 23, 27]  1     20055  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [136, 272, 544]]\n",
      "YOLOv5shuf_dw_peam summary: 286 layers, 4564047 parameters, 4564047 gradients, 7.6 GFLOPs\n",
      "\n",
      "YOLOv5shuf_dw_peam summary: 286 layers, 4564047 parameters, 4564047 gradients, 7.6 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "from models.yolo import Model\n",
    "from utils.torch_utils import model_info\n",
    "\n",
    "model = Model('models/yolov5shuf_dw_peam.yaml')\n",
    "model_info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93a58c45-a54b-4106-952d-bd93b1859bfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1       464  models.common.Conv_maxpool              [3, 16]                       \n",
      "  1                -1  1      5040  models.common.MobileNetV3_InvertedResidual[16, 16, 64, 3, 2, True, False]\n",
      "  2                -1  1      5048  models.common.MobileNetV3_InvertedResidual[16, 40, 72, 3, 1, False, False]\n",
      "  3                -1  1     20510  models.common.MobileNetV3_InvertedResidual[40, 40, 120, 5, 2, True, True]\n",
      "  4                -1  1     61148  models.common.MobileNetV3_InvertedResidual[40, 64, 240, 5, 1, True, True]\n",
      "  5                -1  1     98656  models.common.MobileNetV3_InvertedResidual[64, 128, 480, 3, 2, False, True]\n",
      "  6                -1  1    493928  models.common.MobileNetV3_InvertedResidual[128, 256, 672, 3, 1, True, True]\n",
      "  7                -1  1    132352  models.common.DWConv2D                  [256, 512, 1, 1]              \n",
      "  8                -1  1    410656  models.common.SPPF                      [512, 272, 5]                 \n",
      "  9                -1  1    856464  models.common.TransformerEncoder        [272, 272, 8, 1024, 0.1]      \n",
      " 10                -1  1     74800  models.common.DWConv2D                  [272, 272, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    106231  models.common.PEAM                      [336, 272]                    \n",
      " 14                -1  1    334560  models.common.C3                        [272, 272, 1, False]          \n",
      " 15                -1  1     37536  models.common.DWConv2D                  [272, 136, 1, 1]              \n",
      " 16                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 17           [-1, 2]  1         0  models.common.Concat                    [1]                           \n",
      " 18                -1  1     28229  models.common.PEAM                      [176, 136]                    \n",
      " 19                -1  1     84048  models.common.C3                        [136, 136, 1, False]          \n",
      " 20                -1  1     19992  models.common.DWConv2D                  [136, 136, 3, 2]              \n",
      " 21          [-1, 12]  1         0  models.common.Concat                    [1]                           \n",
      " 22                -1  1    156631  models.common.PEAM                      [472, 272]                    \n",
      " 23                -1  1    334560  models.common.C3                        [272, 272, 1, False]          \n",
      " 24                -1  1     76976  models.common.DWConv2D                  [272, 272, 3, 2]              \n",
      " 25           [-1, 9]  1         0  models.common.Concat                    [1]                           \n",
      " 26                -1  1    334148  models.common.PEAM                      [544, 544]                    \n",
      " 27                -1  1   1334976  models.common.C3                        [544, 544, 1, False]          \n",
      " 28      [19, 23, 27]  1     20055  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [136, 272, 544]]\n",
      "yv5_mo summary: 292 layers, 5027008 parameters, 5027008 gradients, 7.9 GFLOPs\n",
      "\n",
      "yv5_mo summary: 292 layers, 5027008 parameters, 5027008 gradients, 7.9 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "model = Model('models/yv5_mo.yaml')\n",
    "model_info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e18bf0cf-7fb1-47fe-ba29-d6a7f0bc83cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1       464  models.common.Conv_maxpool              [3, 16]                       \n",
      "  1                -1  1      5040  models.common.MobileNetV3_InvertedResidual[16, 16, 64, 3, 2, True, False]\n",
      "  2                -1  1      5048  models.common.MobileNetV3_InvertedResidual[16, 40, 72, 3, 1, False, False]\n",
      "  3                -1  1     20510  models.common.MobileNetV3_InvertedResidual[40, 40, 120, 5, 2, True, True]\n",
      "  4                -1  1     61148  models.common.MobileNetV3_InvertedResidual[40, 64, 240, 5, 1, True, True]\n",
      "  5                -1  1     98656  models.common.MobileNetV3_InvertedResidual[64, 128, 480, 3, 2, False, True]\n",
      "  6                -1  1    493928  models.common.MobileNetV3_InvertedResidual[128, 256, 672, 3, 1, True, True]\n",
      "  7                -1  1    132352  models.common.DWConv2D                  [256, 512, 1, 1]              \n",
      "  8                -1  1    410656  models.common.SPPF                      [512, 272, 5]                 \n",
      "  9                -1  1    856464  models.common.TransformerEncoder        [272, 272, 8, 1024, 0.1]      \n",
      " 10                -1  1     74800  models.common.DWConv2D                  [272, 272, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    106231  models.common.PEAM                      [336, 272]                    \n",
      " 14                -1  1    334560  models.common.C3                        [272, 272, 1, False]          \n",
      " 15                -1  1     37536  models.common.DWConv2D                  [272, 136, 1, 1]              \n",
      " 16                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 17           [-1, 2]  1         0  models.common.Concat                    [1]                           \n",
      " 18                -1  1     28229  models.common.PEAM                      [176, 136]                    \n",
      " 19                -1  1     84048  models.common.C3                        [136, 136, 1, False]          \n",
      " 20                -1  1     19992  models.common.DWConv2D                  [136, 136, 3, 2]              \n",
      " 21          [-1, 12]  1         0  models.common.Concat                    [1]                           \n",
      " 22                -1  1    156631  models.common.PEAM                      [472, 272]                    \n",
      " 23                -1  1    334560  models.common.C3                        [272, 272, 1, False]          \n",
      " 24                -1  1     76976  models.common.DWConv2D                  [272, 272, 3, 2]              \n",
      " 25           [-1, 9]  1         0  models.common.Concat                    [1]                           \n",
      " 26                -1  1    334148  models.common.PEAM                      [544, 544]                    \n",
      " 27                -1  1   1334976  models.common.C3                        [544, 544, 1, False]          \n",
      " 28      [19, 23, 27]  1     40110  models.yolo.DecoupledDetect             [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [136, 272, 544]]\n",
      "yv5_mo_dh summary: 300 layers, 5047063 parameters, 5047063 gradients\n",
      "\n",
      "yv5_mo_dh summary: 300 layers, 5047063 parameters, 5047063 gradients\n"
     ]
    }
   ],
   "source": [
    "model = Model('models/yv5_mo_dh.yaml')\n",
    "model_info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9948d5-e188-447b-910f-156394d4be30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
